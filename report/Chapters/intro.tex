\chapter{Introduction}%
\label{intro}

Recently, there has been a revival in the usage of deep
learning\cite{lecun2015deep} techniques, allowing for large advances across
a number of fields including image recognition\cite{krizhevsky2012imagenet},
speech recognition\cite{graves2013speech, hinton2012deep}. Similarly,
there has been recent advancements in the area of reinforcement learning,
where intelligent agents have been able to complete tasks that were once thought
to be too complex for an agent to complete, such as beating humans in the game
Go\cite{silver2016mastering} and more generally such as across a wide range of
Atari 2600 games\cite{mnih2015human}. However, despite these recent advancements,
there are still a number of challenges that prove difficult for current
reinforcement learning algorithms.

This has lead DeepMind\cite{deepmind} to developing the
StarCraft II Learning Environment\cite{vinyals2017starcraft}, a Python
environment for interfacing with the game StarCraft II\cite{pysc2, starcraft2}.
This game was chosen as it involves a number of areas that current reinforcement
learning algorithms struggle with, including partial observation, a large action
space, control of multiple units and the reward is delayed across thousands of
steps, meaning the agent needs to use long-term strategies.

This environment was developed to be used as a test bed for both new learning
techniques, as well as the improvement of new learning techniques. An agent
that is able to intelligently play the mini-games and the full game of
StarCraft II would need to act very intelligently, which has lead to the large
amount of research into the area, both specifically for StarCraft II and
broadly for reinforcement learning.

\section{Aim}

This project aims to prototype an agent that is able to exhibit intelligent
behaviour in the game of StarCraft II\@ (SC2). This will be achieved using
reinforcement learning (RL), that is without using any form of supervised
learning method.

With the recent addition of the StarCraft II Learning Environment (SC2LE),
applying reinforcement learning techniques and similar to SC2 has become
much easier. With the use of SC2LE, the major aspects of the game are
delivered using an easier to use API, rather than having to read the
screen's state using the pixel values. This, along with an input API,
mean that the creating agents on top of SC2 is much easier.

Specifically, this project aims to apply interesting techniques such as
transfer and curriculum learning to the problems defined in the SC2LE\@.
This has interesting research potential and the chance to be very effective
due to the increasing complexity of the challenge mini-games that are defined
in the SC2LE\@.

\section{Objectives}

As the project is broadly very experimental, there is a few objectives that
have been defined, as well as associated deliverables, which are:

\begin{enumerate}
    \item The understanding and clear definition of the project space, that is
        what techniques will be used to tackle the problems, as well as
        metrics to measure the performance of a given prototype.
    \item A prototype (or multiple using various techniques) that exhibit
        intelligent behaviour in some behaviour in a given scenario for
        the game StarCraft II\@.
    \item Performance analysis of the given prototypes, such that their
        effectiveness can be measured and compared to other solutions, both of
        our design and as defined in other papers.
\end{enumerate}

In turn, these objectives lead to the following deliverable:

\begin{enumerate}
    \item A Git repository containing the prototypes and any other associated
        code in getting the agents running.
    \item Associated learnt weights for each of the given agents, such that
        their performance can be easily tested without having to retrain
        the network.
    \item The evaluation of each of the given agents, along side a full report
        detailing the rest of the project.
\end{enumerate}

\section{Scope}

The scope for this project is somewhat restricted, at least compared to the full
scope that could be taken under this project title. StarCraft II is a very complex
game, with a steep learning curve even for a human player, such that the training
of an agent to play the entire game of StarCraft II is infeasible, at least in the
given time-frame. This has instead lead to the using `mini-games' that are defined
in SC2LE, which were created by DeepMind in order to mock small portions of the
game. This means that the full game will not be the main target, but instead
parts of the game that are in a restricted scope, with more well defined targets.
One advantage of this however, is that there is more well defined scores for
these mini-games, such that comparison against other methods and human players is
made easier.

Similarly, there are other methods that could be used to train an agent for this
task, but it was decided to use reinforcement learning, instead of using some
form of supervised learning method. This was chosen out of interest in the
application of reinforcement learning, and it being used more prominently
in this area.
If a supervised method was needed, DeepMind
do provide a number of game replays of professional%
\footnote{\url{https://en.wikipedia.org/wiki/Professional_StarCraft_competition}}
players playing, which could
be used to train agents.

\section{Report Structure}

There is 7 chapters to this report, including this Introduction chapter.

Following this, there is Chapter~\ref{method}, where the methodologies,
approach to development, technologies to be used and the project
schedule will be discussed in further detail.

Chapter~\ref{research} will go into more detail on the research that went into
the project. This will include the theory for Reinforcement Learning and the
associated techniques we expect to use with this, including Q-Learning,
Convolutional Neural Networks and the basics of neural networks as a
whole. This chapter will also include a look into the problems we
expect to face in this project, as well as techniques that have been tried
before on similar projects.

Chapter~\ref{implem} will cover the specific implementation we ended up using
in the project, and justification of the choices made and the problems
encountered during this.

After this, Chapter~\ref{eval_method} will go into detail on the methods we will use
to evaluate the performance of the agents. This will include the actual testing
methodologies, as well as the importance of testing, such as how and why
to test the generalisation of the agent and the subjectivity of measuring
intelligent behaviour.

The evaluation itself will be carried out in Chapter~\ref{eval}.
This will include performance
comparisons across a number of mini-games, as well as other small game types.
Also, it will contain comparisons against the published numbers for the given
games, as a suitable baseline.

Finally, Chapter~\ref{conclusion} concludes the report, and discusses a few
extensions that could be tested. This chapter will also give a retrospective
on the project as a whole, comparing how the project faired against the estimates
made in Chapter~\ref{method}.

