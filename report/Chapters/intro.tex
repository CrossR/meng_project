\chapter{Introduction}%
\label{intro}

Recently, there has been a revival in the usage of deep
learning~\cite{lecun2015deep} techniques, allowing for large advances across a
number of fields including image~\cite{krizhevsky2012imagenet} and speech
recognition~\cite{graves2013speech, hinton2012deep}. Similarly, there have been
recent advancements in the area of reinforcement learning, where intelligent
agents have been able to complete tasks that were once thought to be too complex
for an agent to complete, such as beating humans in the game
Go~\cite{silver2016mastering} and more generally such as across a wide range of
Atari 2600 games~\cite{mnih2015human}. However, despite these recent
advancements, there are still many challenges that prove difficult for current
reinforcement learning algorithms.

Because of this, DeepMind~\cite{deepmind} developed the
StarCraft II Learning Environment\cite{vinyals2017starcraft}, a Python
environment for interfacing with the game StarCraft II\cite{pysc2, starcraft2}.
This game was chosen as it involves some of the areas that current reinforcement
learning algorithms struggle with, including partial observation, a large action
space, control of multiple units and a reward which is delayed across thousands of
steps, meaning the agent needs to use long-term strategies.

This environment was developed to be used as a test bed for both new learning
techniques, as well as the improvement of existing learning techniques. An agent
that is able to intelligently play the mini-games and the full game of
StarCraft II would need to act very intelligently, which has lead to a large
amount of research into the area, both specifically for StarCraft II and
broadly for reinforcement learning.

\section{Aim}

This project aims to prototype an agent that can exhibit intelligent behaviour
in the game of StarCraft II\@ (SC2). This will be achieved using reinforcement
learning (RL), that is without using any form of supervised learning method.

With the recent addition of the StarCraft II Learning Environment (SC2LE),
applying reinforcement learning techniques and similar to SC2 has become much
more accessible. With the use of SC2LE, the significant aspects of the game are
delivered using an easier to use API, rather than having to read the screen's
state using the pixel values. This, along with an input API, results in an
environment that is much easier to interface with than using traditional
computer vision and mouse emulation techniques.

Specifically, this project aims to apply cutting-edge techniques such as
transfer and curriculum learning to the problems defined in the SC2LE\@.  Both
of these learning techniques have interesting research potential and the chance
to be very effective due to the increasing complexity of the challenge
mini-games that are defined in the SC2LE\@.

\section{Objectives}

As the project is very experimental, there are a few objectives that
have been defined, as well as associated deliverables, which are:

\begin{enumerate}
    \item The understanding and precise definition of the project space, that is
        what techniques will be used to tackle the problems, as well as metrics
        to measure the performance of a given prototype.
    \item A prototype (or multiple using various techniques) that exhibit
        intelligent behaviour in some behaviour in a given scenario for
        the game StarCraft II\@.
    \item Performance analysis of the given prototypes, such that their
        effectiveness can be measured and compared to other solutions, both of
        our design and as defined in other papers.
\end{enumerate}

In turn, these objectives lead to the following deliverable:

\begin{enumerate}
    \item A Git repository containing the prototypes and any other associated
        code for getting the agents running.
    \item Associated learnt weights for each of the given agents, such that
        their performance can be easily tested without having to retrain
        the network.
    \item The evaluation of each of the given agents, alongside a full report
        detailing the rest of the project.
\end{enumerate}

\section{Scope}

The scope of this project is somewhat restricted, at least compared to the full
scope that could be taken under this project title. StarCraft II is a very
complicated game, with a steep learning curve even for a human player, such that
the training of an agent to play the entire game of StarCraft II is infeasible,
at least in the given time-frame. This has instead lead to the using
`mini-games' that are defined in SC2LE, which were created by DeepMind to mock
small portions of the game. This means that the full game will not be the
primary target, but instead parts of the game, using a restricted scope, with
more well-defined targets.  One advantage of this, however, is that there are
published scores for these mini-games, such that comparison with other methods
and human players is made easier.

Similarly, other methods could be used to train an agent for this task, but it
was decided to use reinforcement learning, instead of using some form of
supervised learning method. Instead, reinforcement learning was chosen out of
interest in its application, and it being used more prominently in this area.
If a supervised method was needed, DeepMind do provide game replays of
professional%
\footnote{\url{https://en.wikipedia.org/wiki/Professional_StarCraft_competition}}
players playing, which could be used to train agents, which is discussed more in
detail later.

\section{Report Structure}

There are 7 chapters to this report, including this Introduction chapter.
The rest of the chapters are as follows.

Following this, there is Chapter~\ref{method}, where the methodologies, approach
to development, technologies to be used and the project schedule will be
discussed in further detail.

Chapter~\ref{research} will go into more detail on the research that went into
the project. This will include the theory of Reinforcement Learning and the
associated techniques we expect to use with this, including Q-Learning,
Convolutional Neural Networks and the basics of neural networks as a whole. This
chapter will also include a look at the problems we expect to face in this
project, as well as techniques that have been tried before on similar projects.

Chapter~\ref{implem} will cover the specific implementation we ended up using in
the project, and justification of the choices made and the problems encountered
during this.

After this, Chapter~\ref{eval_method} will go into detail on the methods we will
use to evaluate the performance of the agents. This will include the actual
testing methodologies, as well as the importance of testing, such as how and why
to test the generalisation of the agent and the subjectivity of measuring
intelligent behaviour.

The evaluation itself will be carried out in Chapter~\ref{eval}.  This will
include performance comparisons across the many mini-games, as well as other
small game types.  Also, it will contain comparisons against the published
numbers for the given games, as a suitable baseline.

Finally, Chapter~\ref{conclusion} concludes the report and discusses a few
extensions that could be tested. This chapter will also give a retrospective on
the project as a whole, comparing how the project faired against the estimates
made in Chapter~\ref{method}.
