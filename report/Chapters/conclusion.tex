\chapter{Conclusion}%
\label{conclusion}

\section{Expansions of System}

As mentioned throughout the project, there are many things that could be
investigated, and other already included features that could be expanded.

To start, as spoken about briefly before, we could expand the Deep Q network,
such that it has a much richer action space. This would have two advantages.
Firstly, the agent should be able to learn more complex games and be more
accurate, as currently the action space is the main thing limiting the agents
abilities. This should make it score higher and as such, move closer to a more
intelligent and optimal solution. Secondly, this would have the advantage of
allowing the agent to behave more like a human. That is, despite the agents
compound actions giving it an advantage in the learning process, it is also
unrealistic as compared to a human player, who does not have the option of one
action that then performs many actions. Hopefully with a large enough action
space the agent would be able to learn these actions by itself.

Next, the convolutional network could be expanded. Firstly, this should include
the inclusion of non-spatial information such as resource and unit counts, such
that they can help influence decisions. This would bring it in-line with the
`FullyConv' specification that DeepMind speaks about in their paper. Similarly,
after this, the network could be expanded further to include a Long-term short
term memory component, which is the next network DeepMind specified. These two
modifications would allow the network to have more information, as well as
retain information on past decisions to help make the current decision.

As mentioned during the research, a network that uses a teacher/student mechanic
could be used, to see if it helps it learn the more complex mini-games, or also
if it helps time to train on the simpler games. This could be applied to both
networks, to provide similar comparisons as transfer and curriculum learning were
used for here.

Finally, something the networks would potentially also benefit from is using a
later version of the SC2LE, which has pixel level classifications, rather than
the broad feature maps that are available in the most recent version. This could
help the convolutional method most, but would also raise many issues. The
current network architecture performs very well using a very small number of
convolutional layers since the images it uses are feature maps with very
distinct features. If this was instead swapped to pixel level data, then the
network could potentially be able to become more accurate as it would have the
smaller models rather than the blobs produced by the feature maps, but the
network would also need to be expanded suitably to be able to process the images
rather than feature maps. This adds to the training difficulty of such a network
and would require better feature detection to begin identifying different
objects. This is due to objects being able to overlap in the game, since the
game is a perspective
projection and is angled to the viewer. Meaning that 2 units can partially

Any of the features mentioned here could provide interesting further work for
a project, with the chance of also increasing the performance of the networks
outlined here.

\section{Deliverables}

In Chapter~\ref{intro} a number of objectives and deliverables were defined, and
can now be assessed. The objectives the project set out to achieve are as
follows:

\begin{enumerate}
    \item The understanding and precise definition of the project space, that is
        what techniques will be used to tackle the problems, as well as metrics
        to measure the performance of a given prototype.
    \item A prototype (or multiple using various techniques) that exhibit
        intelligent behaviour in some behaviour in a given scenario for
        the game StarCraft II\@.
    \item Performance analysis of the given prototypes, such that their
        effectiveness can be measured and compared to other solutions, both of
        our design and as defined in other papers.
\end{enumerate}

And the deliverables outlined were:

\begin{enumerate}
    \item A Git repository containing the prototypes and any other associated
        code for getting the agents running.
    \item Associated learnt weights for each of the given agents, such that
        their performance can be easily tested without having to retrain
        the network.
    \item The evaluation of each of the given agents, alongside a full report
        detailing the rest of the project.
\end{enumerate}

Now at the conclusion of the project, these objectives can be reflected upon. Of
the three objectives outlined, all of them were met. This includes a full
chapter outlining both the current methods used for reinforcement learning in
games, as well as the theory behind the techniques. This was used as the basis
for the associated prototypes that were then made for the second objective. This
ended up being two main prototypes, with various tweaks applied to both to
optimise performance. The effectiveness and subjective intelligence of these
agents depends on the scenario at hand, but each are able to act intelligently
in given situations. Finally, to prove the intelligence of the agents beyond
simply subjectively watching them play, performance metrics were gathered from
the mini-games and Simple64 map, such that the performance could be more easily
compared. These scores were compared against published scores when possible, and
also subjectively analysed to ensure they were acting intelligently.

Similarly, the deliverables outlined were all met. All code throughout the
project was kept in a code repository to track changes that were made to the
agents over time, and also to allow easier collaboration between members. The
learnt weights are stored separately due to size and not being suitable to store
inside of version control. Finally, a full performance evaluation and report was
given containing all the research, implementation and evaluation that went into
the project, as well as methodologies for performing these steps where
appropriate.


\section{Conclusion}

In conclusion, this project achieved the aims that it set out to reach,
producing agents that were able to exhibit intelligent behaviour in the game
StarCraft II, and also experiment with the usage of transfer and curriculum
learning with them.

The project focused on the implementation and feasibility of 2 agent
architectures. The main focus was on the different aspects of running these 2
architectures with the usage of curriculum and transfer learning. We were able
to implement and design 2 networks that would play and learn from an in-game
environment. The most difficult aspect of the implementations was in the reward
engineering of a real-time game. This required the most time in development and
was never a complete aspect, as more possible functions could be created and
tested to improve the performance of the agents. However, since the agent needed
to transfer and learn from different maps, the reward needed to be generalised
so that the agent could use the same function on all the mini-games.

Using PySC2 and TensorFlow also introduced some difficulties with developing the
agents. Some of the runs gave internal errors from the TensorFlow API and the
usage of PySC2 required more research due to the poor documentation on the API's
implementation and feature system.

That said, despite the setbacks two agents were made that provide a useful
baseline for further work, as both a comparison target and as an example of how
a StarCraft II agent should look. This should allow further work to be performed
more easily, allowing research to happen faster.
