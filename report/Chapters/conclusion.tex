\chapter{Conclusion}%
\label{conclusion}

\section{Expansions of System}

As mentioned throughout the project, there are many things that could be
investigated, and other already included features that could be expanded.

To start, as spoken about briefly before, we could expand the Deep Q network,
such that it has a much richer action space. This would have two advantages.
Firstly, the agent should be able to learn more complex games and be more
accurate, as currently the action space is the main thing limiting the agents abilities. This should make it score higher and as such, move closer to a more
intelligent and optimal solution. Secondly, this would have the advantage of
allowing the agent to behave more like a human. That is, despite the agents
compound actions giving it an advantage in the learning process, it is also
unrealistic as compared to a human player, who does not have the option of one
action that then performs many actions. Hopefully with a large enough action
space the agent would be able to learn these actions by itself.

Next, the convolutional network could be expanded. Firstly, this should include
the inclusion of non-spatial information such as resource and unit counts, such
that they can help influence decisions. This would bring it in-line with the
`FullyConv' specification that DeepMind speaks about in their paper. Similarly,
after this, the network could be expanded further to include a Long-term short
term memory component, which is the next network DeepMind specified. These two
modifications would allow the network to have more information, as well as
retain information on past decisions to help make the current decision.

As mentioned during the research, a network that uses a teacher/student mechanic
could be used, to see if it helps it learn the more complex mini-games, or also
if it helps time to train on the simpler games. This could be applied to both
networks, to provide similar comparisons as transfer and curriculum learning were
used for here.

Finally, something the networks would potentially also benefit from is using a
later version of the SC2LE, which has pixel level classifications, rather than
the broad feature maps that are available in the most recent version. This could
help the convolutional method most, but would also raise many issues. The
current network architecture performs very well using a very small number of
convolutional layers since the images it uses are feature maps with very
distinct features. If this was instead swapped to pixel level data, then the
network could potentially be able to become more accurate as it would have the
smaller models rather than the blobs produced by the feature maps, but the
network would also need to be expanded suitably to be able to process the
images rather than feature maps. This adds to the training difficulty of such a network and would require better feature detection to begin identifying different objects. Objects can also overlap in the game, since the game is a perspective projection and is angled to the viewer. Meaning that 2 units can partially overlap each other and would require the agent to identify those cases.

Any of the features mentioned here could provide interesting further work for
a project, with the chance of also increasing the performance of the networks
outlined here.

\section{Deliverables}

In Chapter~\ref{intro} a number of objectives and deliverables were defined, and
can now be assessed. The objectives the project set out to achieve are as
follows:

\begin{enumerate}
    \item The understanding and precise definition of the project space, that is
        what techniques will be used to tackle the problems, as well as metrics
        to measure the performance of a given prototype.
    \item A prototype (or multiple using various techniques) that exhibit
        intelligent behaviour in some behaviour in a given scenario for
        the game StarCraft II\@.
    \item Performance analysis of the given prototypes, such that their
        effectiveness can be measured and compared to other solutions, both of
        our design and as defined in other papers.
\end{enumerate}

And the deliverables outlined were:

\begin{enumerate}
    \item A Git repository containing the prototypes and any other associated
        code for getting the agents running.
    \item Associated learnt weights for each of the given agents, such that
        their performance can be easily tested without having to retrain
        the network.
    \item The evaluation of each of the given agents, alongside a full report
        detailing the rest of the project.
\end{enumerate}

Now at the conclusion of the project, these objectives can be reflected upon. Of
the three objectives outlined, all of them were met. This includes a full
chapter outlining both the current methods used for reinforcement learning in
games, as well as the theory behind the techniques. This was used as the basis
for the associated prototypes that were then made for the second objective. This
ended up being two main prototypes, with various tweaks applied to both to
optimise performance. The effectiveness and subjective intelligence of these
agents depends on the scenario at hand, but each are able to act intelligently
in given situations. Finally, to prove the intelligence of the agents beyond
simply subjectively watching them play, performance metrics were gathered from
the mini-games and Simple64 map, such that the performance could be more easily
compared. These scores were compared against published scores when possible, and
also subjectively analysed to ensure they were acting intelligently.

Similarly, the deliverables outlined were all met. All code throughout the
project was kept in a code repository to track changes that were made to the
agents over time, and also to allow easier collaboration between members. The
learnt weights are stored separately due to size and not being suitable to store
inside of version control. Finally, a full performance evaluation and report was
given containing all the research, implementation and evaluation that went into
the project, as well as methodologies for performing these steps where
appropriate.


\section{Conclusion}
%TODO: Final conclusion of the paper, what did we learn, how did it go, what can
%be done to improve it and so on.

Neural networks are one of the main topics of machine learning in today's society. The implementation of these networks allow complicated domain spaces to be assessed and provide an ever increasing possibility to the representational space. The main reason a Deep Q network would be used over a tabular Q-Learning is in the size of the action and state space. As more states and actions get added to the agent the conventional approach will require much more memory and will increase the table for each new entry. To deal with this issue, a neural network will only need to add an input node for each new state information and an output node for each new action. This allows huge combinations of states and actions to be represented without the huge memory requirements of the tabular approach.

The project focused on the implementation and feasibility of 2 agent architectures. The main focus was on the different aspects of running these 2 architectures with the usage of curriculum and transfer learning. We were able to implement and design 2 networks that would play and learn from an in-game environment. The most difficult aspect of the implementations is in the reward engineering of a real-time game. This required the most time in development and was never a complete aspect, as more possible functions could be created and tested to improve the performance of the agents. However, since the agent needed to transfer and learn from different maps, the reward needed to be generalised so that the agent could use the same function on all the mini-games.

The PySC2 and tensorflow also introduced some difficulties with developing the agents. Some of the runs gave internal errors from the tensorflow API. The usage of PySC2 required more research due to the poor documentation on the the API's implementation and feature system.
