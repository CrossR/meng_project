\chapter{Research}
\label{research}

\section{Theory}
A game of Starcraft 2 includes many challenges for a player in terms of complexity. The game state can be comprised of thousands of different aspects that could represent important information for the player. In order for the agent to be able to act, it must be given an input, stimulus of some form, to evaluate and proceed with taking an action. This makes the game ideal for an unsupervised learning environment.
The environment is broken into states. Each state represents information about the game at a given moment. This is then fed into the neural network and returns a value outcome of what the optimal action to be taken would be.
A different approach would be to use a Q-learning algorithm with a table format for storing actions and values. The main difficulty would be the size of the given table. As more actions are introduced the size of the table would increase and new values would need to be optimized for the given cells. Using a neural network, this issue can be avoided by simply adding new weights and nodes to the network.

\section{Challenges}
% Add more info here about the tech


\subsection{PySC2 API}
The agent takes an action based on the given information about the current game state, the contextual environment. This relies on how the information can be extracted using the \textbf{PySC2} API.
In order to make the agent as close to a human player as possible, the API uses an orthogonal projection to view the game world, as opposed to how a human would see perspective projection that is tilted to the side. This mainly due to the issue of overlapping units, where an agent would not be able to distinguish them apart. The agent would have a 84x84 grid view of the current screen. The API also provides access to the player minimap. The Minimap provides:
\begin{itemize}
\item The entire map view including unexplored areas (blacked out)
\item All units and building in the game
\item Objects represented by an index for classification
\end{itemize}

The classifications of minimap objects are:
\begin{enumerate}
\item Background
\item Agent's units
\item Allied units
\item Neutral units
\item Hostile units
\end{enumerate}

The entire game state is accessed using the \textbf{Observation} method provided in the API. In order to access any information from the game, the must be called with the relevant index since the \textbf{Obs} variable is an array of arrays with relevant information. For example, in order to access the minimap, use $obs.observation['screen'][UNITTYPE]$. The agent can see that a unit is hid through the minimap but can not identify the type of unit unless the screen array is accessed for unit type information. The screen array provides the following:
\begin{itemize}
\item Powers of a selected unit
\item Unit type
\item Hit points of a selected unit
\item Unit density (how many units are in a given area of screen)
\item and more information on character abilities
\end{itemize}
For an action to be taken, a return call is made with the required action for the step. This means that the action must be coded to select the relative unit and return the coordinates for selecting the unit and issuing an action. There are 2 types of actions:
\begin{itemize}
\item Queued: an action that will take place once the current action is finished
\item Immediate: interrupts the current action being carried out and forces the character to begin the returned action.
\end{itemize}
Some actions will require an extra parameter for position. This makes the action space much larger as the agent could choose any given position to issue the action in a 84x84 grid. So an action such as building a simple building would have 84x84 possible positions to represent the single action of building.

\section{Reinforcement Learning For Games}
% The earlier sections should cover the general tech,
% whereas this section will talk about it applied to games
% and the unique problems that raises.
The biggest challenge is identifying what information the agent would need such that the input to the network is:
\begin{itemize}
\item Useful in many states
\item Critical to a given optimal decision
\item Contextual
\item Provides a baseline for what a unique state would look like
\end{itemize}
Once a state can be defined, the actions available to an agent will determine its impact on the game world. The agent must be able to take actions that allow it to transition between states with the ability to advance out of a loop created by switching between the same states.
This leads to what action in a given state is optimal and how can the agent learn whether that was the correct decision made. The learning algorithm should allow an agent to identify that remaining in a given loop is not the correct action and that learning new actions could be the better outcome. Since this is using a state action pair the agent will not attempt to take any other actions that have not been tested and defined to be the highest output.

\section{Existing Methods}
% This should then move on from the previous section
% to show how the outlined challenges are dealt with.

\section{Conclusion}

