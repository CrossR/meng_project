\chapter{Research}%
\label{research}

\section{Theory}
% Brief explanation of why we needed to do so much research, as well
% as why it is so important for our style of project
% i.e. Exploratory Software

\subsection{StarCraft II}
% What is StarCraft II?

% Issues with StarCraft II
% Potentially, some of this can be moved to the implementation
% chapter too.
A game of StarCraft II includes many challenges for a player in terms of complexity.
The game state can be comprised of thousands of different aspects that could represent
important information for the player. In order for the agent to be able to act,
it must be given an input, stimulus of some form, to evaluate and then proceed with
taking an action. This makes the game ideal for an unsupervised learning environment.

The environment is broken into states. Each state represents information about
the game at a given moment. This is then fed into the neural network and
returns a value outcome of what the optimal action to be taken would be. A
different approach would be to use a Q-learning algorithm with a table format
for storing actions and values. The main difficulty would be the size of the
given table. As more actions are introduced the size of the table would
increase and new values would need to be optimized for the given cells. Using a
neural network, this issue can be avoided by adding new weights and nodes to the
network.

\subsection{Q-Learning Table}

Q-Learning is a reinforcement learning algorithm that uses a tabular approach to
state action pairs. Q-Learning is an off-policy method which gives the agent the
ability to randomly select actions in a given state without affecting the
updated value of the state action pair. Q-Learning requires:

\begin{itemize}
    \item State: some form of representation of the environment the agent is in.
    \item Action: an action that may move the agent from one state to the next or keep it in the same state.
\end{itemize}

The agent is given a state of the world and checks the lookup table for which
action would have the highest value in that state. The agent then executes that
action checks the table again for the new state. Upon executing an action the
agent may be given a reward, a value that dictates whether the action taken was
favourable or incorrect. The table can be updated through a real-time learning
or at the end of an episode. An episode represents an entire list of states and
actions taken to reach the end, usually by having a terminal state. The table is
updated using the Q-Learning equation.

% Should we add more info about how q learning works and the equation it uses 

\subsection{Neural Networks}

A neural network uses weights which are multiplied by the given value to
represent an activation. This activation works by inputting the given value into
an activation function that will return a value to be used as input for the next
layer. The network is broken into nodes and layers. Each node represents a
variable for input and each layer is a collection of nodes. Layers are connected
through edges known as weights.

\subsection{Deep Q Networks}
The main issue with using a tabular approach for keeping track of state-action values is the number of required entries. As a more complex space needs to be represented, the number of entries in the table would increase for every possible combination of values. The newest approach to tackling the issue of complex space environments is to use a neural network. The weights represent what an expected value for a given state could be. The state is given as input into the network and then the highest value output is chosen as an action to be taken by the agent. The steps for running the network:
\begin{enumerate}
\item A state is inputted into the network.
\item Input is multiplied by the respective connected weights.
\item The next layer is then given the input from the previous layer.
\item The final output layer will return different values for each node in the layer.
\item The node with the highest value represents the action to be taken.
\item Once the action is taken, a reward value is recorded and used to update the network.
\end{enumerate}
The main difference between a Deep Q Network and a normal neural network is the update function used. 
A Deep Q Network uses the loss function from the Q-Learning algorithm to update the weights of the network. The loss function provides a value of the target return and the predicted return. For example, the agent could choose that in a certain state the best action would be the third action with a value of 20. The agent takes the action and gets a returned reward that is less. The network is then updated to return a lesser value for that action next time the agent is in that state again. This is known as the TD error:
\begin{align}
TD Error = \sqrt{(Q(s',a) - Q(s,a))^{2}}
\end{align}
The error is the value difference between the next state reward and the current expected reward. This tells the network whether the expected reward was correct or far. The value is then updated by increasing or decreasing based on the returned reward. \cite{pandey2010reinforcement}

The update could be applied in real-time or at the end of every episode.
When using a real-time learning environment the agent is more likely to mistake an action for being optimal from the first reward it gets. This makes the agent tend to be more bias towards actions that get an immediate return and will not allow the agent to see the possibilities of foreshadowing what could be a better action in the long run. To avoid over fitting for a single environment, the agent needs to be exposed to differing maps and states. Updating the network at the end of each episode, may make the agent learn action pairs that are incorrect and do not impact the reward factor. For example, the agent can move left, which lets assume is correct, then go right and then left again. This introduces a loop that the agent can get stuck in or an extra action step that is unnecessary. 

In order to avoid such an outcome, a discount factor $\gamma$ is used to reduce the reward value for an incoming state. So the more steps an agent may take to the reward, the lesser the reward value is. 
Once the agent takes an action, based on the state given to the network, the reward is observed and used to update the Q-Target. The Q-Target looks at the next state s' and checks which action yields the highest value return. That value is then used to update the current state in the network by using the TD Error.
\begin{align}
QTarget = r + \gamma*Q(s',a)
\end{align}
The prediction made by the network is then subtracted according to the TD Error equation. A learning rate is then multiplied to the value.
Final update value is:
\begin{align}
Q(s,a) = Q(s,a) + \alpha (TDError)
\end{align}

The equation above is the same equation used in the Q-Learning algorithm. But is implemented through the network with a gradient descent optimizer. The network evaluates the loss function using the TD Error and then updates the relevant weights. The learning rate $\alpha$ must be tested with multiple values. If $\alpha$ is increased then the network may converge to an optimal value quicker but may also tend to overshoot the local minimal and diverge from the optimal value. This makes $\alpha$ an important hyper-parameter based on the amount of training the agent may have. If the $\alpha$ is set to a lower value then the agent will require more training to achieve an accurate estimate of the expected reward for a given state action pair. 

\subsection{Convolutional Neural Networks}
A Convolutional Neural Network (CNN) is a type of Artificial Neural Network
(ANN). Broadly, a CNN is similar to that of an ANN, in that it consists of a
number of neurons that have associated weights and receive some input, be that
from an input layer or another layer of neurons. Where they differ though is
that with a CNN it is assumed that the input is an image, and then because of
this, the learning procedure and design principles changes.

To give an example of why this style of network is needed, the size of a non-CNN
network must be considered. For the MNIST~\cite{lecun2010mnist} dataset, the
supplied images are $28 \times 28$. For a fully connected architecture, this
leads to $28 * 28 = 784$ weights. If an RGB image is used, this number is
multiplied by 3 to get the number of pixels in each of the 3 channels. $784 * 3$
weights is reasonable, but once moving to an image of a more reasonable size,
say $(250, 250, 3)$, the number of weights becomes very large, without even
considering the rest of the neurons in the network.

A CNN work differently however. Since it is able to assume the input is an
image, the network architecture may be built in a more specific way. That is,
instead of input consisting on a flat vector of numbers, the input may be
considered in 3D, where the images vertical and horizontal resolution make up
the width and height of the input, and the depth is made up by the channels the
image is made up of, traditionally 3. This is then helped further since the
neurons in this 3D layer are only connected to a small receptive field from the
input image, rather than the entire input like in a fully-connected network.
Broadly, it can be said that a convolutional layer takes a 3D input and
transforms it to some 3D output. In the context of the MNIST dataset, this
would be the classification. An example of this 3D network can be seen in
Figure~\ref{fig:cnn}. It can be seen that the image is given and is transformed
into a second 3D volume of the same size, but with additional channels.

The additional channels once the convolutional layer has been processed is due
to the number of filters in that layer. These are defined as the area the
neuron in a given layer connects to. For example a neuron in the first layer
may look at a $3 \times 3$ section of the input, extending across all channels
of the input. This filter is applied to the entire image by moving it over every
pixel combination it fits on, and at each location the dot product on the input
and the values in the filter is computed. For the whole image $I$ and a filter
$F$, this is defined as follows:

\begin{align}
    {(I*F)}_{xy} = \sum^{h}_{i=1} \sum^{w}_{j=1} F_{ij} \cdot I_{x+i-1, y+j-1}
\end{align}

which means that for a given a $(x,y)$ position, the value of the filter at that
point for a given filter is the dot product of each value in the input and the
value in the filter.

These filters are stacked, which then increases the receptive field of the later
layers. This process was mirrored on features found in
biology~\cite{hubel1968receptive}. The receptive field refers to the area that a
given neuron is receiving input from. For the first layer, this is simply the
size of the filter, for example the $3 \times 3$ grid. However, this layer is
then used as input for the next layer. It can be said that the second layer's
receptive field relative to the image is now 

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{cnn}
    \caption{Common layout of a convolutional neural network, from
    Cambridge Spark\cite{cnn-layout}.}%
    \label{fig:cnn}
\end{figure}

\section{Challenges}
% Add more info here about the tech and the challenges it faces
% i.e. the problems in Reinforcement Learning as a whole

\section{Reinforcement Learning For Games}
% The earlier sections should cover the general tech,
% whereas this section will talk about it applied to games
% and the unique problems that raises.

\section{Existing Methods}
% This should then move on from the previous section
% to show how the outlined challenges are dealt with.

\section{Conclusion}

