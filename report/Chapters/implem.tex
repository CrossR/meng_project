\chapter{Implementation \& Design}%
\label{implem}

\section{StarCraft II API and SC2LE}

The SC2LE is an environment produced by DeepMind to provide a Python wrapper for
the StarCraft II API that is produced by Blizzard. This API allows both
information to be received from the game, as well as the sending of input to the
game. This is made even easier with the SC2LE, due to it serving up this API in
a language that fits directly with the Machine Learning libraries (Python, using
TensorFlow), as well as providing many different views of both the screen and
the mini-map, as can be seen in Figure~\ref{fig:sc2le}. In it, the main portion
of the screen shows the current screen and in the bottom left, the current
mini-map. The right-hand half of the screen has different views of both of the
screen and mini-map, showing different information in each.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{pysc2}
    \caption{Example of the StarCraft II Learning Environment interface.}%
    \label{fig:sc2le}
\end{figure}

Using the SC2LE provides an easy interface to the game, such that game states
can be read, and input actions can be sent. An agent takes an action based on
the given state, which in turn relies on how the information can be extracted
from the SC2LE\@.

To make the agent as close to a human player as possible, the API uses a grid to
represent most of the required information into a multiple array structure, where
each sub-array is concerned with a single aspect of the game. The arrays mostly
represent a grid-like structure, which can be treated as a simplified version of
the pixels that make up the game. Each sub-array is a column value with another
array to represent the row. This means that the agent would see information
about the world using a sampling of the available pixels.

An example would be the \texttt{screen} array which contains the columns and
sub-arrays that represent the row value. By indexing this array twice with
\texttt{`screen[x][y]'}, it is possible to get the value associated with grid
position \texttt{(x,y)}. The agent by default has a $64 \times 64$ grid view of
the current screen, and a second view of the same size of the mini-map, though
this can be changed using run-time flags.

The API implements an orthogonal projection to view the game world, as opposed
to how a human would see a perspective projection that is tilted to the side.
This is mainly due to the issue of overlapping units, where an agent would not
be able to distinguish them apart.

The API also provides access to the player mini-map. The mini-map provides:

\begin{itemize}
    \item The entire map view including unexplored areas, which are blacked out.
    \item All units and building in the game.
    \item Objects represented by an index for classification.
\end{itemize}

The classifications of the mini-map objects are:
\begin{enumerate}
    \item Background
    \item Agent's units
    \item Allied units
    \item Neutral units
    \item Hostile units
\end{enumerate}

The entire game state is accessed using the \texttt{observation} method provided
by the SC2LE\@. To access any information from the game, the user must index the
object with the relevant index since the \texttt{obs} variable is an array of
arrays with pertinent information. For example, to access the units of a given
type on the screen currently, the following code is used,
\texttt{obs.observation['screen'][UNITTYPE]}. The agent can see that a unit is
hidden through the mini-map but cannot identify the type of unit unless the
screen array is accessed for unit type information.

The screen array provides the following:

\begin{itemize}
    \item Powers of a selected unit
    \item The unit type
    \item Hit points of a selected unit
    \item Unit density (how many units are in a given area of the screen)
    \item More information on character abilities
\end{itemize}

For an action to be taken, a return call is made with the required action for
the step. This means that the action must be coded to select the relative unit
and return the co-ordinates for selecting the unit and issuing an action. This
makes most common actions in the game take multiple in-game steps, resulting in
even a common action such as movement being comprised of a selection step and
then an order to move.

There are two types of actions:

\begin{itemize}
    \item Queued: An action that will take place once the current
        action is finished.
    \item Immediate: Interrupts the current action being carried out and
        forces the character to begin the returned action.
\end{itemize}

Some actions require an extra parameter for position. This makes the action
space much more significant as the agent could choose any given position to
issue the action in a $64 \times 64$ grid. So an action such as building a
simple building would have $64 \times 64$ possible positions to represent the
single action of building. However, this is somewhat reduced due to some of the
positions being invalid for a given action, for example, a building can not be
built on top of an existing building.

\section{Problem Space}
Since the game is very complicated and can consist of many possible action spaces,
the majority of the experimentation focuses on the DeepMind \textbf{mini-games}
and simple small games in the \textbf{Simple64} map. A more thorough description
of these maps will be given in Chapter~\ref{eval_method}.

The differences between these two challenges mean that the networks will need to
work for both or be easily swappable between them. This is because the
mini-games define custom scoring routines, which allows the map to give a score
on the completion of an objective, rather than finishing the map as a whole.

This does, however, help reduce the scope of the agents that are being designed,
as the game state is reduced significantly compared to the playing of the full
game. This is even truer if the agent was being built to play against a human
player, rather than the in-game AI or with no opponent, as is the case in the
mini-games.

\section{Development Choices}

As part of building an intelligent agent, or working with any form of deep
learning application, many parameters can be tuned that will alter the
performance of the network for a given task.

The most prominent challenge was identifying what information the agent would
need such that the input to the network was:

\begin{itemize}
    \item Useful in many states
    \item Critical to a given optimal decision
    \item Contextual
    \item Provides a baseline for what a unique state would look like
\end{itemize}

Once a state can be defined, the actions available to an agent will determine
its impact on the game world. The agent must be able to take actions that allow
it to transition between states with the ability to advance out of a loop
created by switching between the same states. This is what makes the choice of
the input state so crucial.

If given insufficient information, the agent could potentially make undesirable
choices due to not having enough information and performing the best action that
was visible.

This leads to what action in a given state is optimal and can the agent learn
whether the correct decision was made. The learning algorithm should allow an
agent to identify that remaining in a given loop is not the correct action and
that learning new actions could be the better outcome. Since this is using a
state action pair, the agent will not attempt to take any other actions that
have not been tested and defined to be the highest output.

\section{Deep Q Network}

For the agent to be able to act, it must be given an input, stimulus of some
form, to evaluate and then proceed with taking action. This makes the game
ideal for an unsupervised learning environment.

The environment is broken into states. Each state represents information about
the game at a given moment. This is then fed into the neural network and
returns a value outcome of what the optimal action to be taken would be. A
different approach would be to use a Q-learning algorithm with a table format
for storing actions and values. The main difficulty would be the size of the
given table. As more actions are introduced the size of the table would
increase, and new values would need to be optimised for the given cells. Using a
neural network, this issue can be avoided by adding new weights and nodes to the
network.

The following section will focus on the Deep Neural Network that implements a
Q-Learning update function and policy evaluation.

\subsection{Network Architecture}

The network used for running the agent is comprised of the following layers:

\begin{enumerate}
    \item Input layer with 37 inputs for the state representation.
    \item First hidden layer with 50 nodes and a bias node.
    \item Second hidden layer with 40 nodes and a bias node.
    \item Third hidden layer with 25 nodes and a bias node.
    \item Output layer with 20 nodes (one for each possible action).
\end{enumerate}

The weights of each of the layers use a random normal distribution with a
standard deviation of 0.5 and a mean of 0. This allows the network to randomly
set the initial weights without the weights going beyond the range of -2 to +2.
If the weights are initialised to values that are greater than two, then the
network would overshoot in its calculations and assign incorrect values to the
weights with each update step. This then causes the network to tend to infinite
values since the TD error is always positive. The initial implementation used
random distributions without setting the standard deviation and caused the
network to overflow in values and tend to infinity. Since each layer will depend
on the previous multiplication and summation of the previous nodes when the
network was increased in size the values needed to be truncated to reduce the
weights from becoming too large.

The input may also need to be reduced due to the weight multiplication. In this
case, the input values were multiplied by 0.001 to minimise the overall
estimation of the network. This also requires the rewards to be reduced for the
TD Error evaluation.

\subsection{Exploration}

The agent uses an Epsilon-greedy algorithm for exploration. The algorithm works
by setting a value $\epsilon$ and taking the maximum value action for a given
state with a probability of $1-\epsilon$. Otherwise, the agent will take a random
action in that state. Using the Epsilon-greedy algorithm the agent can
learn the best action for a given state by randomly sampling different actions.
This helps the agent escape from the same loop of choosing the same action for a
given state.

The value of $\epsilon$ reduces with each time the agent takes a random action
so that the number of times the agent runs a random action is reduced. This is
known as Epsilon-decay function. The value of the decay is set before running
the network and is multiplied by the current $\epsilon$ every time a random
action is taken. The value of $\epsilon$ is therefore reduced with every
multiplication.

\begin{align}
    \epsilon = \epsilon \times decay
\end{align}

The initial value of $\epsilon$ is set to a high value such as 0.4. After a couple
of random actions taken the value of $\epsilon$ will reach a threshold value
(usually 0.1) and will no longer be multiplied by the decay factor. The agent
will then continue running with a fixed value of $\epsilon$ for the remainder of
the episodes. This allows the agent to start by testing more actions in
different states and eventually choosing the highest value action more often.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{random_action}
    \caption{Agent takes an action and repeats.}%
    \label{fig:random_action}
\end{figure}

\subsection{Reward}

The agent can use a terminal reward system, where the final total reward is
given without live rewards, or live immediate reward system, where the agent is
given the reward during runtime. When using a terminal reward the agent will
receive the final reward without any immediate reward for what action in that
state may have achieved. Since the network implements a reward decay $\gamma$,
the agent will learn what pattern of actions and states helped to achieve that
final reward. This could lead to the problem of the agent over-fitting and
learning only a given pattern to take. However, the mini-games used for
evaluation will randomise the game state with every run. For example, the
MoveToBeacon map will randomly move the beacon to a different area of the map
every time. This makes it impossible for the agent to learn to move in a
specific pattern and achieve the same result. This does, however, take longer for
the agent to converge on the correct actions to take but will make the agent
increase the total reward at the end of each episode. If the agent is given an
immediate reward, then the agent may try to achieve the same reward every time
without knowledge of how well it has performed overall. In some cases using a
live reward system may be favourable for the agent, but this would depend on the
task and requirement of the agent~\cite{shelton2001balancing}.

The agent's ability to learn the correct actions to take will depend on the
reward system. If the agent takes an action, the reward will define the value of
that taken action. This introduces the problem with live learning in a game that
runs in real-time. Since the game has actions that can take time to perform,
attacking, for example, the agent will get the reward in the next few steps.
This means that the agent could take the correct action and then take another
action after it. The reward will go to the later action taken, but that is not
the action that was the correct one to take. For example, the agent decides to
attack an enemy unit and will receive a reward once the unit is dead, which
takes multiple attacks and is not instantaneous. The agent then takes the action
to build something, and at that given state the enemy unit was killed. This
would confuse the agent into assuming that the building action was the correct
action and not the attacking action. Using $\gamma$ can help rectify this issue.
The reward decay allows the agent to plan a couple of steps from the taken
action. The higher $\gamma$ is, the more it adds to the current state and action
value. Allowing the agent to associate higher values for the given state will
give better future outcomes, rather than the agent looking for the immediate
reward. Equation~\ref{eq:q_update} shows $\gamma$ being multiplied by the next
state's value based on the best action to be taken next.

\subsection{Negative Reward}

The agent will need to learn the correct action sequence to take to increase the
overall score of the mini-games. However, in between actions can cause the agent
to learn incorrect loops. Since the game has an animation time, which is the
time the game takes to do an animation after the agent has taken an action, the
agent will call other actions in this time frame without affecting the
previously taken action. For example, say the agent attacks the top right corner
and the unit is on the opposite side of the map. The unit will need to walk to
the other side of the map and within this window of walking and attacking the
agent can call other actions, which may not cancel the attacking action
previously called. In this case, the agent will learn an incorrect sequence of
actions that will affect the reward taken as the agent will learn that the
reward was for the in-between actions and not the original ones.

A negative reward can fix such behaviours and prevent the agent from staying in a
looped sequence of actions. The negative reward will be smaller than the
positive reward so that the agent does not remain in his place with a 0 value.
In some cases, the agent will learn the correct sequence of actions without the
need for a negative reward. Using a negative reward also allows the agent to
find a route to the solution with the least cost as the agent moves in a grid.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{negative_reward}
    \caption{Game world with negative and positive rewards for reaching the beacon.}%
    \label{fig:negative_reward}
\end{figure}

Figure~\ref{fig:negative_reward} shows how the agent will receive a negative
reward if it takes more actions to reach the goal. This will also stop the agent
from looping by going left and right multiple times before it goes for the goal. 

Since each incorrect action will have its weights reduced, the trained model will have to be retrained on the new mini-game since the previous training session reduced the expected action. For example, the agent is trained to play the MoveToBeacon mini-game. When the agent is training, any actions such as building will be useless as there is no reward for building in the MoveToBeacon map. The agent is then moved to the BuildMarines mini-game, where the expected action is to build. The agent will not build due to the weights of those actions being small because of the previous training session. This is reduced by improving the state space so that the agent has enough information to distinguish the differences in actions needed.

\subsection{Learning}

The Deep Q network can be updated at the end of or during a game. Using an
offline network allows the agent to evaluate itself over all the choices made by
the agent during gameplay. The state, action, reward and next state are saved
into an array for updating the network once the episode is done. However, the
issue with offline learning is that the agent will not receive a reward for the
given action at the correct time and may require the agent be stopped for a
couple of steps.

\subsubsection{Off-Policy Learning}

Q-learning is an off-policy learning algorithm. An off-policy method does not
require the policy behaviour to evaluate the new expected value. The agent can
take any form of action and update the relevant Q-value after the action
has been taken. The method works by taking a greedy step, where the agent's next
state is evaluated for the highest value action in the next state. An on-policy
method would make it difficult for the agent to take a random action that is not
the action the agent would usually take. Since on-policy requires the agent to
take a greedy step, the value of that step would be updated as opposed to the
random action the agent would have taken. For example, if the highest value for
the agent is to go up but the agent takes a random action of down and receives a
reward. During the learning phase, the agent will update the up action with the
reward from the action of going down.

Using an off-policy method, the agent accumulates the states, actions and
rewards into an array. By taking random actions, the agent can explore and later
update the Q-values without updating the wrong action. Off-policy methods can
learn the optimal policy without the values from the behaviour policy.

\subsubsection{Memory}

An array known as a ``memory'' is used for storing the state, action and rewards
from the episode. The memory array is then shuffled, which helps avoid learning
certain patterns or over-fitting. The memory array allows the agent to play
without losing any of the steps it took. Once the episode is finished, a
terminal state contains the final overall score of the agent.

\subsubsection{The Remember function}

The remember function is called at the end of every episode and triggers the
updating of the Deep Q network. The memory array is shuffled and sampled from one
array at a time. The array needs to be sampled in these single steps so that the
network can be run to test the next state prediction from the given state. The
function takes a single array that contains the state, action, reward and next
state from a single game step. The action is used to check the next state's
value. Once the network has been run on the next state, a value is returned
representing the next state's expected return. The loss function can then be
calculated using these two values.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{agent_learn}
    \caption{The agents learning process by using a memory array.}%
    \label{fig:agent_learn}
\end{figure}

\section{Convolutional Neural Network}

This section will focus on the Convolutional Neural Network implementation that
was used, as based on the DeepMind specification.

\subsection{Network Architecture}

The convolutional network used is built of the following layers and a
visualisation of this can be seen in Figure~\ref{fig:cnn_arch}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{cnn-arch}
    \caption{An overview of the architecture used for the CNN.}%
    \label{fig:cnn_arch}
\end{figure}

\begin{enumerate}
    \item This following process is applied to both the mini-map and the screen
        in parallel, though the input of the mini-map is different as it does
        not contain any unit type data, as this is not available via the
        mini-map.
        \begin{enumerate}
            \item Use the player relative screen or mini-map (usually a $32
                \times 32$ array), as input to a convolutional layer. Before
                inputting, the input is `one hot encoded' to swap it from being
                in the range 0--5 to binary form. From here, this information is
                combined with the unit types list (for the screen) and the
                non-player relative screen information (for both the screen and
                the mini-map). All of this is passed to a single convolutional
                layer with 16 filters, using a kernel of size $(5 \times 5)$, a
                ReLU activation function, and a stride of 1. Padding is applied
                such that the input is the same size as the output.
            \item This first convolutional layer leads into a second with 32
                filters, a kernel of size $(3 \times 3)$, and the same
                activation function and padding.
    \end{enumerate}
    \item The two convolutional layers from the screen and mini-map are then
        combined into a single \texttt{visual\_input} variable, which is then
        used to provide input to two more layers.
    \begin{enumerate}
        \item The first of these layers is a convolutional layer. This layer
            consists of a single output that the spatial action is based upon,
            with a kernel of $(1 \times 1)$ and no activation function. This
            output is converted to a probability by using the \texttt{softmax}
            function. Essentially, this means this layer is used to generate a
            probability distribution over the entire input, such that area with
            the maximum probability can be used as the co-ordinate for the
            spatial action.
        \item The second of these layers is a fully-connected layer. This takes
            the \texttt{visual\_input} variable and flattens it down, such that
            it can be used as input to a fully connected layer with 256 outputs,
            with a ReLU activation function. This then leads into a second
            fully-connected layer that has number of outputs equal to the
            number of possible actions, as defined in the SC2LE\@. This uses no
            activation function and is used to calculate the action to take.
            This vector of action probabilities is then corrected such that any
            unavailable actions have a probability of zero, with the remaining
            values updated to sum to 1 again.
    \end{enumerate}
    \item A value estimate is generated by taking the first fully-connected
        layer and using that as an input to a second fully-connected layer with
        a single output, and squeezing the layer (that is, remove dimensions of
        size 1) to get a value estimate.
    \item Both the spatial action and action id probabilities are
        logarithmically clipped, to restrict their size, before being finally
        passed back to be acted upon.
\end{enumerate}

This network architecture is similar to that outlined in the DeepMind SC2LE
paper, but differs in one crucial way, in that it is solely based upon screen
inputs, whereas the model outlined in the paper also combines some non-spatial
features into the model as well. The exact values used are not discussed but it
most likely contains non-spatial values such as the current amount of resources,
the number of units and more that can be accessed through the \texttt{obs} object.
This input was omitted to make the initial model easier to build, and due to
difficulty in working out suitable values to pass over, as well as where they
made the most sense to include.

\subsection{Learning}

The agent trains live, in that it builds up a list of states as it plays, and
trains on those states during a game, rather than waiting until the end of the
game. This process works as follows:

\begin{enumerate}
    \item For the current game state in a single frame, the agent takes a step
        using the CNN, which is used to generate an action, co-ordinate and value
        tuple based on the game state of that frame. This is done by taking the
        \texttt{obs} object and using it to run the network defined earlier.
        This produces an action ID, a spatial action location and an estimate of
        the value of the given action at the specified location. These values
        are stored in an array, along with the full \texttt{obs} object.
    \item Once all these values have been stored, the predicted action and
        co-ordinate pair is processed, such that the action is taken. The game
        then progresses a number of frames, and the actual reward from that
        action is processed and stored.
    \item This process continues for a number of steps, usually eight, before the
        network then starts to evaluate the results. If the episode ends in the
        middle of this, the final score is logged, and then the results are
        evaluated. The learning works by first calculating the advantage that
        was gained, by comparing the estimate and the actual rewards. Next, this
        advantage is combined with the following:
        \begin{itemize}
            \item The value target for each of the eight steps.
            \item The taken action and co-ordinate pair for each of the eight steps.
            \item The \texttt{obs} object for each of the eight steps.
        \end{itemize}
        This then leaves an object that contains all the information for the
        eight taken steps, including the screen state, the action/co-ordinate
        pair, the estimated value and the actual reward.
    \item This object containing all the eight steps is passed into the
        \texttt{train} function, so that the weights of the network can be
        updated.  This time, the network runs a loss optimisation function,
        rather than simply sampling the current values to get out some
        probabilities. The network attempts to minimise the following loss,
        using the Adam optimiser, defined in TensorFlow.  Alongside this,
        logging is done for easier reviewing later.
    \item Finally, the \texttt{obs} object is updated one last time so the next
        batch starts with a new object, and the batch counter is incremented.
\end{enumerate}

The training operation itself is using the TensorFlow \texttt{optimize\_loss}
layer to optimise the \texttt{total\_loss} variable, which is made up of the
following components:

\begin{itemize}
    \item The policy loss, which is made up of the mean of the logarithmic
        probabilities of the actions combined with the gained advantage.
    \item The value loss, which is the mean squared error of the value target
        and estimate. This is then multiplied by some value, to weight this loss
        lower or higher.
    \item The negative spatial action entropy, is the sum of the spatial action
        probabilities with their logarithmic probabilities. This value is then
        multiplied by some value, to weight it again.
    \item Finally, similar to the negative spatial action entropy the negative
        action entropy is calculated with the mean of the summed action
        probabilities and the logarithmic action probabilities. Similarly, this
        is also multiplied by a weight value.
    \item These four values are added together into the single
        \texttt{total\_loss} variable, which is minimised.
\end{itemize}

These values are also all logged out as part of this process, such that the loss
can be mapped through the training process, to see how both the score for the
game improves, and how the loss is minimised throughout.
