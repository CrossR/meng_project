\chapter{Implementation \& Design}%
\label{implem}

\section{Problem Space}
Since the game is very complex and can consist of many possible action spaces,
the majority of the experimentation focuses on the DeepMind mini-games
and simple small games in the \textbf{Simple64} map.

The mini-games consist of small constrained challenges that represent
small parts of the full game of StarCraft II, allowing simple actions
in the game to be tested, as well as compared against a range of
published scores.

The \textbf{Simple64} map was also chosen to allow experimentation on
a very simple version of the full game, where the challenge is
to beat the games AI, which is built using more traditional game AI
technology, and also can cheat at higher difficulties.

The map contains a single opponent and 2 starting bases.
The agent is spawned into one of the 2 areas randomly and starts out
with the \textbf{SCV} units, characters for building and mining.
The initial stage begins with mining of necessary of resources for progressing
through level by creating the required buildings and training \textbf{Marines}, fighter characters.
This requires the agent to control the \textbf{SCV} and \textbf{Marine} units
using complex three action commands.

\section{PySC2 API}

% Possibly needs a broader explanation of what this is, and why we use it.

The agent takes an action based on the given information about the current game
state, the contextual environment.
This relies on how the information can be extracted using the \textbf{PySC2} API\@.
In order to make the agent as close to a human player as possible, the API uses an grid to represent most of the required information into a multiple array structure where each sub-array is concerned with a single aspect of the game. The arrays mostly represent a grid like structure. Each sub-array is a column value with another array to represent the row. This means that the agent would see information about the world using a sampling of the available pixels. An example would be the \textbf{screen} array which contains the columns and sub-arrays that represent the row value. By indexing this array twice \textbf{"screen[x][y]"} it is possible to get a value associated with grid position \textbf{(x,y)}. The agent would have a $84 \times 84$ grid view of the current screen.

The API implements an orthogonal projection to view the game world, as opposed to how a human would see a perspective projection that is tilted to the side. This is mainly due to the issue of overlapping units, where an agent would not be able to distinguish them apart.


The API also provides access to the player mini-map. The mini-map provides:

\begin{itemize}
    \item The entire map view including unexplored areas (blacked out)
    \item All units and building in the game
    \item Objects represented by an index for classification
\end{itemize}

The classifications of mini-map objects are:
\begin{enumerate}
    \item Background
    \item Agent's units
    \item Allied units
    \item Neutral units
    \item Hostile units
\end{enumerate}

The entire game state is accessed using the \textbf{Observation} method provided in the API\@.
In order to access any information from the game, the must be called with the
relevant index since the \textbf{Obs} variable is an array of arrays with
relevant information. For example, in order to access the mini-map,
use $obs.observation['screen'][UNITTYPE]$. The agent can see that a
unit is hid through the mini-map but can not identify the type of
unit unless the screen array is accessed for unit type information.

The screen array provides the following:

\begin{itemize}
    \item Powers of a selected unit
    \item Unit type
    \item Hit points of a selected unit
    \item Unit density (how many units are in a given area of screen)
    \item and more information on character abilities
\end{itemize}

For an action to be taken, a return call is made with the required action for the step.
This means that the action must be coded to select the relative unit and return the coordinates
for selecting the unit and issuing an action. There are 2 types of actions:

\begin{itemize}
    \item Queued: an action that will take place once the current
        action is finished
    \item Immediate: interrupts the current action being carried out and
        forces the character to begin the returned action.
\end{itemize}

Some actions require an extra parameter for position.
This makes the action space much larger as the agent could choose any given
position to issue the action in a $84 \times 84$ grid.
So an action such as building a simple building would have $84 \times 84$
possible positions to represent the single action of building.

\section{Development Choices}

As part of building an intelligent agent, or working with any form of deep learning application,
there are a number of parameters that can be tuned, that will alter the networks performance
for a given task.

The biggest challenge was identifying what information the agent would need such that the
input to the network was:

\begin{itemize}
    \item Useful in many states
    \item Critical to a given optimal decision
    \item Contextual
    \item Provides a baseline for what a unique state would look like
\end{itemize}

Once a state can be defined, the actions available to an agent will determine its
impact on the game world. The agent must be able to take actions that allow it to
transition between states with the ability to advance out of a loop created
by switching between the same states. This is what makes the choice of the input
state so crucial.

If given insufficient information, the agent could potentially make undesirable
choices due to not having enough information and performing the best
action that was visible.

This leads to what action in a given state is optimal and can the agent learn whether
the correct decision was made. The learning algorithm should allow an agent
to identify that remaining in a given loop is not the correct action and that learning
new actions could be the better outcome.
Since this is using a state action pair the agent will not attempt to take any
other actions that have not been tested and defined to be the highest output.

\section{DeepQ Network}
The following section will focus on the Deep Neural Network that implements a Q-Learning update function and policy evaluation.
\subsection{Network Architecture}
The network used for running the agent is comprised of the following layers:
\begin{enumerate}
\item Input layer with 37 inputs for the state representation
\item First hidden layer with 50 nodes and a bias node
\item Second hidden layer with 40 nodes and a bias node
\item Third hidden layer with 25 nodes and a bias node
\item Output layer with 20 nodes (one for each possible action)
\end{enumerate}
The weights of each of the layers uses a random normal distribution with a standard deviation of 0.5 and a mean of 0. This allows the network to randomly set the initial weights without the weights going beyond the range of -2 to +2. If the weights are initialized to values that are greater than 2 then the network would overshoot in its calculations and assign incorrect values to the weights with each update step. This then causes the network to tend to infinite values since the TD error is always positive. The initial implementation used random distributions without setting the standard deviation and caused the network to overflow in values and tend to infinity. Since each layer will depend on the previous multiplication and summation of the previous nodes, when the network was increased in size the values needed to be truncated to reduce the weights from becoming too large.

The input may also need to be reduced due to the weight multiplication. In this case, the input values were multiplied by 0.001 to minimize the overall estimation of the network. This also requires the rewards to be reduced for the TD Error evaluation.

\subsection{Exploration}
The agent uses an Epsilon-greedy algorithm for exploration. The algorithm works by setting a value $\epsilon$ and taking the maximum value action for a given state with a probability of $1-\epsilon$. Otherwise the agent will take a random action in that state. Using the Epsilon-greedy algorithm the agent is able to learn the best action for a given state by randomly sampling different actions. This helps the agent escape from the same loop of choosing the same action for a given state. 

The value of $\epsilon$ reduces with each time the agent takes a random action so that the number of times the agent runs a random action is reduced. This known as Epsilon-decay function. The value of the decay is set prior to running the network and is multiplied by the current $\epsilon$ every time a random action is taken. The value of $\epsilon$ is therefore reduced with every multiplication.
\begin{align}
\epsilon = \epsilon \times decay
\end{align}

The initial value of $\epsilon$ is set to high value such as 0.4. After a couple of random actions taken the value of $\epsilon$ will reach a threshold value (usually 0.1) and will no longer be multiplied by the decay factor. The agent will then continue running with a fixed value of $\epsilon$ for the remainder of the episodes. This allows the agent to start by testing more actions in different states and eventually choosing the best action more often. 

\subsection{Reward}
The agent can use a terminal reward system, where the final total reward is given without live rewards, or live immediate reward system, where the agent is given the reward during run time. When using a terminal reward the agent will receive the final reward without any immediate reward for what action in that state may have achieved. Since the network implements a reward decay $\gamma$, the agent will learn what pattern of actions and states helped to achieve that final reward. This could lead to the problem of the agent over-fitting and learning only a given pattern to take. However, the mini-games used for evaluation will randomize the game state with every run. For example the MoveToBeacon map will randomly move the beacon to a different area of the map every time. This makes it impossible for the agent to learn to move in a specific motion and achieve the same result. This does, however, take longer for the agent to converge on the correct actions to take but will make the agent increase the total reward at the end of each episode. If the agent is given an immediate reward than the agent may try to achieve the same reward every time without knowledge of how well it has performed overall. In some cases using a live reward system may be favorable for the agent, but this would depend on the task and requirement of the agent. \cite{shelton2001balancing}