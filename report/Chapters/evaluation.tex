\chapter{Performance Evaluation}%
\label{eval}

This chapter, the performance of both of the networks outlined in
Chapter~\ref{implem} will be evaluated, such that their performance at playing
the game StarCraft II can be measured. The exact methodology for these tests is
described in Chapter~\ref{eval_method}.

Each section will be split into three, first evaluating the Deep Q network,
second the Convolutional network, and then finally comparing the two. The
results will mainly focus around the score achieved, which is the in-game is the
final score for the mini-game at hand, and for the Simple64 map this will
compare how the bot fairs across 5 games against various bots. For the
mini-games the overall comparison will allow comparisons with known baselines,
such that we can show how the agents stack up. For the Simple64 game where this
is not possible, it will instead focus on how the agent plays.

%TODO: I just made up the number here for how we will test the Simple64 map...
%so we should actually pick a number and make sure to update the above bit.

After this has been done for both the mini-games and the Simple64 map, the
results will compared overall to see what the advantages and disadvantages of
each network is, as well as comparing other parts of the networks such as
training time, and robustness.

Finally, before the conclusion, some of the additional parts of the trained
networks which are not directly linked to the performance of the network, but
do give insight into how the networks works will be shown. For example the
filters for the Convolutional network will be given here, as well as some
example outputs.

\section{Mini-games}

The mini-games are specific maps designed for training and testing different
skill requirements from the player. Each mini-game uses a different scoring
system, map and end goal. This makes it ideal for testing the agent on different
abilities for the agent to learn. Most of the mini-games incorporate some form
of randomness so that the agents do not learn to play using the same fixed
move-set. Because of this, the mini-games will be played 200 times with a
trained agent, and the mean and max scores will be recorded. This will allow a
suitable average to be taken, which takes into account the inherent randomness
of the mini-games.

For comparison, the results achieved by DeepMind are given in
Table~\ref{tab:deepmind}, as taken from the ``StarCraft II:\@ A New Challenge
for Reinforcement Learning'' paper.

\begin{table}[h]
    \centering
    \begin{tabular}{@{}c|c|rrrrrrr@{}}
        Agent & Metric &
        \rot{MoveToBeacon} & \rot{CollectMineralShards} &
        \rot{FindAndDefeatZerglings} & \rot{DefeatRoaches} &
        \rot{DefeatZerglingsAndBanelings} & \rot{CollectMineralsAndGas} &
        \rot{BuildMarines} \\ \midrule

        \multirow{2}{*}{Random Policy} & Mean & 1 & 17 & 4 & 1 & 23 & 12 & \textless{}1 \\
                                       & Max & 6 & 35 & 19 & 46 & 118 & 750 & 5 \\ \midrule

        \multirow{2}{*}{Random Search} & Mean & 25 & 32 & 21 & 51 & 55 & 2318 & 8 \\
                                       & Max & 29 & 57 & 33 & 241 & 159 & 3940 & 46 \\ \midrule

        \multirow{2}{*}{DeepMind Human Player} & Mean & 26 & 133 & 46 & 41 & 729 & 6880 & 138 \\
                                               & Max & 28 & 142 & 49 & 81 & 757 & 6952 & 142 \\ \midrule

        \multirow{2}{*}{StarCraft GrandMaster} & Mean & 28 & 177 & 61 & 215 & 727 & 7566 & 133 \\
                                               & Max & 28 & 179 & 61 & 363 & 848 & 7566 & 133 \\ \midrule \midrule

        \multirow{2}{*}{Atari-Net} & Best Mean & 25 & 96 & 49 & 101 & 81 & 3356 & \textless{}1 \\
                                   & Max & 33 & 131 & 59 & 351 & 352 & 3505 & 20 \\ \midrule

        \multirow{2}{*}{FullyConv} & Best Mean & 26 & 103 & 45 & 100 & 62 & 3978 & 3 \\
                                   & Max & 45 & 134 & 56 & 355 & 251 & 4130 & 42 \\ \midrule

        \multirow{2}{*}{FullyConv LSTM} & Best Mean & 26 & 104 & 44 & 98 & 96 & 3351 & 6 \\
                                        & Max & 35 & 137 & 57 & 373 & 444 & 3995 & 62
    \end{tabular}
    \caption{Scores achieved and outlined in the DeepMind ``StarCraft II:\@ A New
    Challenge for Reinforcement Learning'' paper.}%
    \label{tab:deepmind}%
\end{table}

\subsection{Deep Q Network}
%TODO: Evaluate all mini-games under DQN. As there is 8 I wouldn't go into
%detail for them all, mainly maybe 2 successes and 2 failures? The interesting
%ones basically. Give table of all results. Try not to do anything on curriculum
%learning etc.

Starting off with the Deep Q network, the mini-games that are to be used must
correlate with the network's state inputs. Since this network does not use an
image as the entire state input for the game, the mini-games chosen need to
relate the states that the network is designed to use. An example would be the
ability to move individual units. The Deep Q network does not yet support the
ability to move individual units since this is not in the networks available actions and
requires a much more complex actions to control individual units. This means
that the agent may perform sub-optimally in certain games.
On the other hand, due to the networks action space including actions that are
built up of multiple actions, this does also mean that in certain games the
agent has an advantage in that it can take a complicated action easily.

Results are given in Table~\ref{tab:dqn_results}, where the mean and max score
achieved is given, as well as the number of episodes the agent was trained for
to achieve that score.

\begin{table}[h]
    \centering
    \begin{tabular}{@{}lrrr@{}}
        \toprule
        Map                         & Mean & Max & Episode Count \\ \midrule
        MoveToBeacon                &      &     &               \\
        CollectMineralShards        &      &     &               \\
        FindAndDefeatZerglings      &      &     &               \\
        DefeatRoaches               &      &     &               \\
        DefeatZerglingsAndBanelings &      &     &               \\
        CollectMineralsAndGas       &      &     &               \\
        BuildMarines                &      &     &               \\ \bottomrule
    \end{tabular}
    \caption{Results for the Deep Q Network}%
    \label{tab:dqn_results}%
\end{table}


\subsection{Convolutional Network}
%TODO: Evaluate all mini-games under CNN. As there is 8 I wouldn't go into
%detail for them all, mainly maybe 2 successes and 2 failures? The interesting
%ones basically. Give table of all results. Try not to do anything on curriculum
%learning etc.

In contrast to the Deep Q network, the Convolutional network does not have to
consider the state inputs, due to the only input being the screen and mini-map
data. This is both an advantage and disadvantage for the network, as it does not
need tuning potentially, but does also mean that the network lacks the required
information to perform well in some of the mini-games. For example, some
mini-games require the usage of resource management and knowing when to spend
resources to build units or buildings. Since the current number of resources is
not shown on the screen to the agent, it is unable to make decisions based on
this and as such suffers in these mini-games.
Similarly, there is no issues with action space due to having access to the
entire game action space. This means the agent is able to move multiple units
independently, and perform many actions. However, as there is no compound
actions, it is much harder for the network to perform complex actions, which
puts it at a disadvantage in later mini-games.

As before, Table~\ref{tab:cnn_results} contains the mean and max
scores achieved, as well as the number of episodes the agent trained for.

\begin{table}[h]
    \centering
    \begin{tabular}{@{}lrrr@{}}
        \toprule
        Map                         & Mean & Max & Episode Count \\ \midrule
        MoveToBeacon                & 26 & 32 & 2000 \\
        CollectMineralShards        &      &     &               \\
        FindAndDefeatZerglings      &      &     &               \\
        DefeatRoaches               &      &     &               \\
        DefeatZerglingsAndBanelings &      &     &               \\
        CollectMineralsAndGas       &      &     &               \\
        BuildMarines                &      &     &               \\ \bottomrule
    \end{tabular}
    \caption{Results for the Convolutional Network}%
    \label{tab:cnn_results}%
\end{table}

\subsection{Network comparison}
%TODO: Compare/Contrast - Why did X network do well on some mini-game whilst the
%other did badly? Then spin it around and show why that helped it do well on a
%different one whilst the other struggled. I expect this will mainly compare the
%compound actions of the DQN vs the simple actions of the CNN.

\section{The Simple64 Map}
%TODO: Tiny bit on what, like 1 or 2 lines.

\subsection{Deep Q Network}
%TODO: Evaluate a number of games of the simple64 map under the DQN network.
%Give table of results, then a play by play of why it did good or bad across the
%games. Try not to do too much on the mini-games and then S64, since that is
%covered later.

\subsection{Convolutional Network}
%TODO: Evaluate a number of games of the simple64 map under the CNN network.
%Give table of results, then a play by play of why it did good or bad across the
%games. Try not to do too much on the mini-games and then S64, since that is
%covered later.

\subsection{Network comparison}
%TODO: Compare the networks again, explain why this time one did well and one
%did badly. Maybe talk about consistency here, how often they won, how their in
%game strategies compare etc.

\section{Transfer and Curriculum Learning}
%TODO: Add a few examples of the above, and show how it changes the results.
% I.e. show a bot going from MoveToBeacon -> CollectMinerals ->
% FindAndDefeatZerglings and compare that to learning with FindAndDefeatZerglings only.
% Similarly, compare learning a/many mini-games and then the Simple64 map as
% well as only the Simple64 map.
% Compare how well it does, time to learn, etc.

\section{Overall comparison}
%TODO: Overall comparison over both Accuracy and Generalisation and the networks as a
% whole, probably also including mention of training time.

\section{Network Information}
%TODO: Other additional parts of the network that hasn't been covered yet but
%wasn't applicable to the implem section. CNN - Filter displays etc. DQN - could
%we see how the QTable changes? etcetc

\section{Conclusion}
%TODO: Broad conclusions of the networks based on the their performances.
