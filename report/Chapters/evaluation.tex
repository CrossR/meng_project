\chapter{Performance Evaluation}%
\label{eval}

This chapter, the performance of both of the networks outlined in
Chapter~\ref{implem} will be evaluated, such that their performance at playing
the game StarCraft II can be measured. The exact methodology for these tests is
described in Chapter~\ref{eval_method}.

Each section will be split into three, first evaluating the Deep Q network,
second the Convolutional network, and then finally comparing the two. The
results will mainly focus around the score achieved, which is the in-game is the
final score for the mini-game at hand, and for the Simple64 map this will
compare how the bot fairs across 5 games against various bots. For the
mini-games the overall comparison will allow comparisons with known baselines,
such that we can show how the agents stack up. For the Simple64 game where this
is not possible, it will instead focus on how the agent plays.

%TODO: I just made up the number here for how we will test the Simple64 map...
%so we should actually pick a number and make sure to update the above bit.

After this has been done for both the mini-games and the Simple64 map, the
results will compared overall to see what the advantages and disadvantages of
each network is, as well as comparing other parts of the networks such as
training time, and robustness.

Finally, before the conclusion, some of the additional parts of the trained
networks which are not directly linked to the performance of the network, but
do give insight into how the networks works will be shown. For example the
filters for the Convolutional network will be given here, as well as some
example outputs.

\section{Mini-games}

The mini-games are specific maps designed for training and testing different skill requirements from the player. Each mini-game uses a different scoring system, map and end goal. This makes it ideal for testing the agent on different abilities for the agent to learn. Most of the mini-games incorporate some form of randomness so that the agents do not learn to play using the same fixed move-set.

\subsection{Deep Q Network}

Starting off with the Deep Q network, the mini-games that are to be used must correlate with the network's state inputs. Since this network does not use an image as the entire state input for the game, the mini-games chosen need to relate the states that the network is designed to use. An example would be the ability to move individual units. The Deep Q network does not yet support the ability to move individual units since this is not in it's available actions and require a much greater complexity to of available actions. Instead the agent will be tested on it's ability to issue a command for attacking a hostile unit using the entire army at the agents disposal.



%TODO: Evaluate all mini-games under DQN. As there is 8 I wouldn't go into
%detail for them all, mainly maybe 2 successes and 2 failures? The interesting
%ones basically. Give table of all results. Try not to do anything on curriculum
%learning etc.

\subsection{Convolutional Network}
%TODO: Evaluate all mini-games under CNN. As there is 8 I wouldn't go into
%detail for them all, mainly maybe 2 successes and 2 failures? The interesting
%ones basically. Give table of all results. Try not to do anything on curriculum
%learning etc.

\subsection{Network comparison}
%TODO: Compare/Contrast - Why did X network do well on some mini-game whilst the
%other did badly? Then spin it around and show why that helped it do well on a
%different one whilst the other struggled. I expect this will mainly compare the
%compound actions of the DQN vs the simple actions of the CNN.

\section{The Simple64 Map}
%TODO: Tiny bit on what, like 1 or 2 lines.

\subsection{Deep Q Network}
%TODO: Evaluate a number of games of the simple64 map under the DQN network.
%Give table of results, then a play by play of why it did good or bad across the
%games. Try not to do too much on the mini-games and then S64, since that is
%covered later.

\subsection{Convolutional Network}
%TODO: Evaluate a number of games of the simple64 map under the CNN network.
%Give table of results, then a play by play of why it did good or bad across the
%games. Try not to do too much on the mini-games and then S64, since that is
%covered later.

\subsection{Network comparison}
%TODO: Compare the networks again, explain why this time one did well and one
%did badly. Maybe talk about consistency here, how often they won, how their in
%game strategies compare etc.

\section{Transfer and Curriculum Learning}
%TODO: Add a few examples of the above, and show how it changes the results.
% I.e. show a bot going from MoveToBeacon -> CollectMinerals ->
% FindAndDefeatZerglings and compare that to learning with  FindAndDefeatZerglings only.
% Similarly, compare learning a/many mini-games and then the Simple64 map as
% well as only the Simple64 map.
% Compare how well it does, time to learn, etc.

\section{Overall comparison}
%TODO: Overall comparison over both Accuracy and Generalisation and the networks as a
% whole, probably also including mention of training time.

\section{Network Information}
%TODO: Other additional parts of the network that hasn't been covered yet but
%wasn't applicable to the implem section. CNN - Filter displays etc. DQN - could
%we see how the QTable changes? etcetc

\section{Conclusion}
%TODO: Broad conclusions of the networks based on the their performances.
